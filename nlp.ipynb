{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading texts...\n",
      "Loaded in 9.35 seconds\n",
      "Cleaning texts...\n",
      "Cleaned in 48.37 seconds\n",
      "Applying normalization...\n",
      "Normalized in 5.80 seconds\n",
      "Applying stemming...\n",
      "Stemmed in 36.07 seconds\n",
      "Tokenizing texts...\n",
      "Tokenized in 13.75 seconds\n",
      "Converting texts to TF-IDF...\n",
      "Converted to TF-IDF in 4.72 seconds\n",
      "Extracting handcrafted features...\n",
      "Handcrafted features extracted in 27.86 seconds\n",
      "Combining features...\n",
      "Combined in 17.72 seconds\n",
      "Selecting top 1000 features...\n",
      "Selected top features in 5.43 seconds\n",
      "Saving data to CSV...\n",
      "Data saved to CSV in 12.13 seconds\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import time\n",
    "\n",
    "# downloaded tokenizer \n",
    "nltk.download('punkt')\n",
    "\n",
    "# Path to the main data folder\n",
    "data_folder = 'Data'\n",
    "\n",
    "# Initialize ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "# Define the stopwords list \n",
    "arabic_stopwords = [\n",
    "    'نيسان', 'هما', 'آه', 'بس', 'أخبر', 'لا', 'مه', 'كل', 'بَسْ', 'إنما', 'ترك', 'لعل', 'إلّا', 'قد', 'ألف', \n",
    "    'أنى', 'بك', 'وَيْ', 'لستما', 'خبَّر', 'ت', 'ذينك', 'أولالك', 'حيَّ', 'نا', 'هَجْ', 'ما', 'سرا', 'قلما', \n",
    "    'وما', 'اتخذ', 'عسى', 'يورو', 'عجبا', 'اللائي', 'إياها', 'جنيه', 'كلما', 'جيم', 'فاء', 'بهن', 'أولاء', \n",
    "    'سين', 'حادي', 'خمسين', 'ض', 'بماذا', 'حزيران', 'شباط', 'مع', 'غير', 'وا', 'كليكما', 'بات', 'ثلاثين', \n",
    "    'تِه', 'هَاتانِ', 'اثنان', 'ه', 'ثمة', 'ست', 'فو', 'لم', 'مائة', 'يفعلان', 'التي', 'زود', 'نعم', 'شمال', \n",
    "    'أسكن', 'ة', 'سحقا', 'ثان', 'ذين', 'تين', 'من', 'ا', 'رزق', 'أنتم', 'هؤلاء', 'تارة', 'عل', 'فمن', 'ثامن', \n",
    "    'كذلك', 'ثمانين', 'أوت', 'رُبَّ', 'إيه', 'لي', 'لبيك', 'ثاء', 'أمسى', 'زعم', 'دولار', 'ثلاثمائة', 'ذلك', \n",
    "    'هذان', 'ذات', 'بخ', 'لكنما', 'هنا', 'هناك', 'ح', 'لمّا', 'جميع', 'بعض', 'ستمائة', 'هَيْهات', 'لكي', 'خمسون', \n",
    "    'جويلية', 'لات', 'ريال', 'هذه', 'خ', 'لهما', 'عاد', 'لست', 'آمينَ', 'وإن', 'كأيّن', 'كلا', 'أقبل', 'إحدى', \n",
    "    'هل', 'اربعون', 'بئس', 'فيما', 'حجا', 'ثلاثون', 'كأنما', 'ثمانمئة', 'اثنا', 'الألى', 'أجمع', 'صبرا', 'كان', \n",
    "    'انقلب', 'مادام', 'لسن', 'تِي', 'عشر', 'نحو', 'فلان', 'بؤسا', 'ذهب', 'بطآن', 'أحد', 'عاشر', 'درى', 'الآن', \n",
    "    'لعمر', 'سوف', 'ّأيّان', 'ولكن', 'أول', 'أطعم', 'ألا', 'صاد', 'اللتيا', 'ء', 'مكانَك', 'كيت', 'صبر', 'ثلاثة', \n",
    "    'جوان', 'ياء', 'عن', 'عند', 'أينما', 'جانفي', 'لما', 'لئن', 'ذَيْنِ', 'فضلا', 'د', 'ءَ', 'كما', 'حيث', \n",
    "    'سادس', 'تانِ', 'سبعمئة', 'أصلا', 'ميم', 'مافتئ', 'أفريل', 'أوشك', 'أبدا', 'كيفما', 'إياه', 'إذن', 'ومن', \n",
    "    'أمام', 'واو', 'يوان', 'ع', 'تسعمائة', 'صهْ', 'مايو', 'لسنا', 'نوفمبر', 'ظاء', 'شَتَّانَ', 'مازال', 'خمسمئة', \n",
    "    'ين', 'إياكن', 'كى', 'أو', 'فإن', 'ن', 'ذانِ', 'ذِه', 'مكانكنّ', 'تلكم', 'أرى', 'ديسمبر', 'شبه', 'كثيرا', \n",
    "    'ثمنمئة', 'تعلَّم', 'غدا', 'غين', 'هَاتِه', 'يناير', 'كلاهما', 'نيف', 'جلل', 'ليسا', 'إياهن', 'اللتين', \n",
    "    'إليكم', 'دونك', 'كأنّ', 'عشرين', 'أيّ', 'ذلكم', 'أي', 'أربعاء', 'سابع', 'أل', 'إيهٍ', 'حتى', 'سبت', \n",
    "    'حبيب', 'خاء', 'هلّا', 'عامة', 'أيضا', 'كسا', 'أى', 'جمعة', 'هاتان', 'ب', 'لوما', 'اللتان', 'أغسطس', \n",
    "    'باء', 'إذما', 'وإذ', 'ص', 'عليه', 'تعسا', 'إمّا', 'ريث', 'قطّ', 'لو', 'أنت', 'ليست', 'ما برح', 'حين', \n",
    "    'ف', 'ضحوة', 'وراءَك', 'عما', 'كن', 'إلَيْكَ', 'لكنَّ', 'خلافا', 'عدا', 'لهن', 'بل', 'هيا', 'ارتدّ', 'أين', \n",
    "    'كرب', 'تسعة', 'نحن', 'تسعمئة', 'فيه', 'لن', 'أُفٍّ', 'إن', 'تفعلين', 'علق', 'هكذا', 'حدَث', 'هَذِه', \n",
    "    'هيت', 'كي', 'ك', 'صباح', 'وجد', 'حمٌ', 'كذا', 'أنتما', 'أنتِ', 'ستون', 'ستين', 'تلقاء', 'إياك', 'تموز', \n",
    "    'أهلا', 'حسب', 'إذ', 'عشرون', 'طَق', 'كانون', 'لكما', 'علم', 'اللذين', 'ثاني', 'ذواتا', 'أمد', 'رابع', \n",
    "    'س', 'لدن', 'شتان', 'عليك', 'كأين', 'أيلول', 'سبعمائة', 'فرادى', 'بغتة', 'قام', 'ؤ', 'أنًّ', 'بين', 'إنا', \n",
    "    'هاته', 'م', 'ضاد', 'تسعين', 'حاي', 'وهو', 'عَدَسْ', 'إليكنّ', 'طاق', 'مذ', 'بكم', 'همزة', 'ثم', 'بعدا', \n",
    "    'إنه', 'والذين', 'فبراير', 'سبعون', 'أيار', 'هنالك', 'آهٍ', 'منذ', 'آها', 'أبٌ', 'راح', 'أولئك', 'بلى', \n",
    "    'تبدّل', 'تسع', 'سبتمبر', 'لا سيما', 'ليرة', 'كلَّا', 'سبعة', 'ذيت', 'حرى', 'له', 'ثمانية', 'سبحان', 'مئة', \n",
    "    'اثني', 'هاكَ', 'كاد', 'أمامك', 'استحال', 'أعطى', 'هاء', 'خال', 'جير', 'أبريل', 'ذا', 'شيكل', 'قبل', \n",
    "    'كِخ', 'الذين', 'بمن', 'غ', 'تفعلون', 'ثالث', 'كم', 'مما', 'أربعمائة', 'ئ', 'تانِك', 'وإذا', 'ش', 'تلكما', \n",
    "    'آذار', 'لكيلا', 'هيّا', 'كيف', 'غالبا', 'لكم', 'إلى', 'خميس', 'هَذِي', 'ته', 'أما', 'في', 'كأي', 'إليكَ', \n",
    "    'هللة', 'خاصة', 'أخذ', 'ثلاثمئة', 'ذِي', 'خلا', 'إذا', 'خلف', 'صار', 'ما أفعله', 'يونيو', 'ولو', 'شين', \n",
    "    'ذي', 'آنفا', 'بنا', 'ثماني', 'لستم', 'تاء', 'بيد', 'إليك', 'ذلكما', 'كلتا', 'هاك', 'آ', 'مكانكما', \n",
    "    'آناء', 'أوّهْ', 'ظ', 'ماي', 'أنشأ', 'سمعا', 'اللاتي', 'نبَّا', 'لستن', 'أكثر', 'أن', 'بهما', 'أفٍّ', \n",
    "    'تجاه', 'اللذان', 'كاف', 'هَذَيْنِ', 'سنتيم', 'بما', 'ط', 'هبّ', 'آض', 'لها', 'أقل', 'ولا', 'لاسيما', \n",
    "    'لعلَّ', 'حمدا', 'عيانا', 'صهٍ', 'مارس', 'نون', 'قاف', 'مئتان', 'خمس', 'أخٌ', 'هَذانِ', 'فلا', 'وهب', \n",
    "    'مرّة', 'ى', 'فيم', 'ليت', 'خمسة', 'نَخْ', 'خامس', 'ستة', 'ذواتي', 'ثمَّ', 'أصبح', 'منه', 'الذي', 'إنَّ', \n",
    "    'ذانك', 'حَذارِ', 'أ', 'سبع', 'هَاتِي', 'هو', 'لولا', 'الألاء', 'ليستا', 'أربع', 'لنا', 'هذي', 'رجع', \n",
    "    'درهم', 'على', 'إما', 'شتانَ', 'تحوّل', 'حاء', 'أجل', 'آهاً', 'ج', 'كلّما', 'ممن', 'اربعين', 'تينك', \n",
    "    'إليكما', 'م', 'إذاً',\"اذا\", 'سرعان', 'سقى', 'تخذ', 'أبو', 'أمامكَ', 'هي', 'إيانا', 'هَؤلاء', 'بسّ', 'ذال', \n",
    "    'يفعلون', 'عدَّ', 'آهِ', 'ما انفك', 'عين', 'و', 'قاطبة', 'أنّى', 'أربعة', 'راء', 'دون', 'هاتي', 'ها', \n",
    "    'منها', 'ثمّ', 'أنتن', 'واهاً', 'بها', 'سوى', 'ر', 'ثلاثاء', 'طالما', 'ابتدأ', 'يوليو', 'مليم', 'رويدك', \n",
    "    'أيها', 'هلم', 'إياهم', 'أمّا', 'هاهنا', 'ذ', 'هيهات', 'هَاتَيْنِ', 'غداة', 'اللواتي', 'لدى', 'ق', \n",
    "    'ساء', 'ثمانون', 'ألفى', 'دينار', 'بكن', 'بَلْهَ', 'أعلم', 'تفعلان', 'أخو', 'صراحة', 'بكما', 'أنا', \n",
    "    'إياكما', 'تَيْنِ', 'هلا', 'أنبأ', 'واحد', 'دال', 'كأن', 'هاتين', 'تسعون', 'مساء', 'مهما', 'زاي', 'ليسوا', \n",
    "    'إياهما', 'يمين', 'اثنين', 'عوض', 'ظنَّ', 'حيثما', 'ذاك', 'أيا', 'علًّ', 'رأى', 'لام', 'طفق', 'بهم', \n",
    "    'ليس', 'كليهما', 'ستمئة', 'أمس', 'ظلّ', 'كأيّ', 'حمو', 'آي', 'أم', 'تاسع', 'صدقا', 'آب', 'انبرى', \n",
    "    'هذين', 'فيها', 'أيّان', 'ذه', 'متى', 'والذي', 'تي', 'هن', 'عشرة', 'طرا', 'حاشا', 'إياي', 'فلس', 'ورد', \n",
    "    'فيفري', 'أكتوبر', 'حار', 'أربعمئة', 'سبعين', 'مكانكم', 'مثل', 'قرش', 'تحت', 'به', 'لكن', 'غادر', 'ي', \n",
    "    'بعد', 'لهم', 'إياكم', 'إليكن', 'تلك', 'ز', 'ل', 'إى', 'نَّ', 'أف', 'طاء', 'هم', 'هَذا', 'ثلاث', 'ذلكن', \n",
    "    'إزاء', 'ذو', 'حبذا', 'ثمان', 'نفس', 'ثمّة', 'معاذ', 'حقا', 'لك', 'تشرين', 'دواليك', 'اخلولق', 'ذوا', \n",
    "    'بضع', 'فوق', 'فإذا', 'شرع', 'ث', 'إي', 'ذان', 'أوه', 'إلا', 'بي', 'أفعل به', 'يا', 'خمسمائة', 'وُشْكَانَ', \n",
    "    'جعل', 'بخٍ', 'أضحى', 'هذا'\n",
    "]\n",
    "# Function to load texts from folders\n",
    "def load_texts_from_folders(data_folder):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # List of categories folders\n",
    "    categories = ['articlesEconomy', 'articlesLocal', 'articlesInternational', 'articlesSports', 'articlesReligion', 'articlesCulture']\n",
    "    \n",
    "    # Loop through the folders for each category\n",
    "    for category in categories:\n",
    "        folder_path = os.path.join(data_folder, category)\n",
    "        \n",
    "        # Load text files from each category\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the contents of each file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                texts.append(text)\n",
    "                labels.append(category)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Function to remove stopwords using the predefined list\n",
    "def remove_stopwords_arabic(text):\n",
    "    words = text.split()\n",
    "    # Remove stopwords using the stopwords list\n",
    "    filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to normalize Arabic texts\n",
    "def normalize_arabic_text(text):\n",
    "    # Replace \"صلى الله عليه وسلم\" with a single token\n",
    "    text = text.replace(\"صلى الله عليه وسلم\", \"صلى_الله_عليه_وسلم\")\n",
    "    \n",
    "    # Remove diacritics \n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
    "    \n",
    "    # Remove both Arabic and English numbers\n",
    "    text = re.sub(r'[0-9٠-٩]+', '', text)\n",
    "    \n",
    "    # Remove all types of Hamza\n",
    "    text = re.sub(r'[إأآءؤئ]', 'ا', text)\n",
    "    \n",
    "    # Remove special symbols and punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to apply stemming using ISRIStemmer\n",
    "def apply_stemming(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Hand-crafted features - only two features\n",
    "def extract_handcrafted_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Feature 1: Number of occurrences of economy related words\n",
    "    economy_words = [\"مال\", \"اقتصاد\", \"سوق\", \"تجارة\"]\n",
    "    num_economy = sum(1 for word in tokens if word in economy_words)\n",
    "    \n",
    "    # Feature 2: Number of unique characters\n",
    "    num_unique_chars = len(set(text))\n",
    "    \n",
    "    return [num_economy, num_unique_chars]\n",
    "\n",
    "# Function to select the top 1000 features\n",
    "def select_top_1000_features(X, y, num_features=1000):\n",
    "    # Ensure that the number of selected features does not exceed 1000\n",
    "    k = min(num_features, X.shape[1])\n",
    "    \n",
    "    # Apply feature selection using Chi-Squared\n",
    "    selector = SelectKBest(score_func=chi2, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# Add a time counter\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the texts\n",
    "print(\"Loading texts...\")\n",
    "texts, labels = load_texts_from_folders(data_folder)\n",
    "print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Clean the texts and remove stopwords\n",
    "start_time = time.time()\n",
    "print(\"Cleaning texts...\")\n",
    "cleaned_texts = [remove_stopwords_arabic(text) for text in texts]\n",
    "print(f\"Cleaned in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Apply normalization to the cleaned texts\n",
    "start_time = time.time()\n",
    "print(\"Applying normalization...\")\n",
    "normalized_texts = [normalize_arabic_text(text) for text in cleaned_texts]\n",
    "print(f\"Normalized in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Apply stemming to the normalized texts\n",
    "start_time = time.time()\n",
    "print(\"Applying stemming...\")\n",
    "stemmed_texts = [apply_stemming(text) for text in normalized_texts]\n",
    "print(f\"Stemmed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert texts into tokens\n",
    "start_time = time.time()\n",
    "print(\"Tokenizing texts...\")\n",
    "tokenized_texts = [tokenize_text(text) for text in stemmed_texts]\n",
    "print(f\"Tokenized in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert the texts to numerical representation using TF-IDF\n",
    "start_time = time.time()\n",
    "print(\"Converting texts to TF-IDF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in tokenized_texts])\n",
    "print(f\"Converted to TF-IDF in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract handcrafted features (only two features)\n",
    "start_time = time.time()\n",
    "print(\"Extracting handcrafted features...\")\n",
    "handcrafted_features = np.array([extract_handcrafted_features(text) for text in cleaned_texts])\n",
    "print(f\"Handcrafted features extracted in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Combine TF-IDF features with handcrafted features\n",
    "start_time = time.time()\n",
    "print(\"Combining features...\")\n",
    "X_combined = np.hstack((X_tfidf.toarray(), handcrafted_features))\n",
    "print(f\"Combined in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Select the top 1000 features\n",
    "start_time = time.time()\n",
    "print(\"Selecting top 1000 features...\")\n",
    "X_new = select_top_1000_features(X_combined, labels, num_features=1000)\n",
    "print(f\"Selected top features in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "start_time = time.time()\n",
    "print(\"Saving data to CSV...\")\n",
    "df = pd.DataFrame(X_new)\n",
    "df['class'] = labels\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "print(f\"Data saved to CSV in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Process completed successfully!\")\n",
    "                              \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9275150629906883\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      articlesCulture       0.91      0.91      0.91       735\n",
      "      articlesEconomy       0.87      0.90      0.88       964\n",
      "articlesInternational       0.95      0.93      0.94       481\n",
      "        articlesLocal       0.85      0.85      0.85      1015\n",
      "     articlesReligion       0.98      0.99      0.99      1051\n",
      "       articlesSports       0.99      0.98      0.98      1231\n",
      "\n",
      "             accuracy                           0.93      5477\n",
      "            macro avg       0.93      0.92      0.93      5477\n",
      "         weighted avg       0.93      0.93      0.93      5477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, labels, test_size=0.3, random_state=1)\n",
    "\n",
    "# Apply SelectKBest to select the top 1000 features using Chi-Squared \n",
    "selector = SelectKBest(score_func=chi2, k=1000)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Define the scaler and scale the selected training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define and train the SVM model with polynomial kernel\n",
    "svm_model = SVC(C=1.0, kernel='poly', degree=1, tol=1e-12, random_state=1)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the SVM model on the test set\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the given text is: articlesSports\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the class of a new text file\n",
    "def predict_text_class(file_path, svm_model, vectorizer, scaler, selector):\n",
    "    # Load the new text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Preprocess the text (same steps as used in training)\n",
    "    cleaned_text = remove_stopwords_arabic(text)\n",
    "    normalized_text = normalize_arabic_text(cleaned_text)\n",
    "    stemmed_text = apply_stemming(normalized_text)\n",
    "    tokenized_text = ' '.join(tokenize_text(stemmed_text))\n",
    "    \n",
    "    # Convert the processed text to numerical representation using TF-IDF\n",
    "    text_tfidf = vectorizer.transform([tokenized_text]).toarray()\n",
    "    \n",
    "    # Extract handcrafted features for the new text\n",
    "    handcrafted_features = np.array(extract_handcrafted_features(cleaned_text)).reshape(1, -1)\n",
    "    \n",
    "    # Combine TF-IDF features with handcrafted features\n",
    "    combined_features = np.hstack((text_tfidf, handcrafted_features))\n",
    "    \n",
    "    # Apply the same feature selection that was used during training\n",
    "    combined_features_selected = selector.transform(combined_features)\n",
    "    \n",
    "    # Normalize the features \n",
    "    combined_features_scaled = scaler.transform(combined_features_selected)\n",
    "    \n",
    "    # Predict the class using the trained SVM model\n",
    "    predicted_class = svm_model.predict(combined_features_scaled)\n",
    "    \n",
    "    return predicted_class[0]\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\DELL\\Desktop\\NLP-assignment\\sp.txt'  # Provide the path to the text file\n",
    "predicted_class = predict_text_class(file_path, svm_model, tfidf_vectorizer, scaler, selector)\n",
    "\n",
    "print(f\"The predicted class for the given text is: {predicted_class}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the SVM model, vectorizer, scaler, and selector to a file\n",
    "with open('svm_model.pkl', 'wb') as file:\n",
    "    pickle.dump((svm_model, tfidf_vectorizer, scaler, selector), file)\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
